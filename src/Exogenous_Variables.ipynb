{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:51:04.069356Z",
     "start_time": "2019-09-01T15:49:22.228823Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import math\n",
    "import copy\n",
    "import multiprocessing\n",
    "import traceback\n",
    "import hashlib\n",
    "import signal\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try: \n",
    "    __file__\n",
    "except:\n",
    "    curr_dir = os.path.abspath('')\n",
    "else:\n",
    "    curr_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    \n",
    "app_root = curr_dir if os.path.basename(curr_dir) != \"src\" else os.path.dirname(curr_dir)\n",
    "\n",
    "if getpass.getuser() == \"rainfalld\":  # docker daemon\n",
    "    home = os.path.expanduser(\"~\")\n",
    "    destdir = home                    # /var/cache/rainfall-predictor\n",
    "else:\n",
    "    destdir = os.path.join(app_root,'data','manipulated_data')      # non-docker stay in repository\n",
    "\n",
    "\n",
    "file = os.path.join(destdir,'rainfalldata.csv')\n",
    "rd = pd.read_csv(file)\n",
    "file2 = os.path.join(destdir,'ncrainfalldata.csv')\n",
    "ncrd = pd.read_csv(file2)\n",
    "rd.Date = pd.to_datetime(rd.Date)\n",
    "rd = rd.set_index('Date')\n",
    "ncrd.Date = pd.to_datetime(ncrd.Date)\n",
    "ncrd = ncrd.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T10:30:04.299825Z",
     "start_time": "2019-08-20T10:30:04.206119Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# this cell takes the stored exogen dictionary that is stored in the Data_Wrangling_CAP1 jupyter notebook\n",
    "# that was imported above.\n",
    "try:\n",
    "    raise NameError()\n",
    "    %store -r exogen\n",
    "except NameError:\n",
    "    f = open(os.path.join(destdir,\"exogen.json\"),\"r\")\n",
    "    exogen = json.load(f)      # read from file, passed from Data_Wrangling\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Library\n",
    "Handles parallel-processing model calculation, mean absolute error calculations, and near-real-time calculation storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:51:04.428724Z",
     "start_time": "2019-09-01T15:51:04.413112Z"
    }
   },
   "outputs": [],
   "source": [
    "def sarima_model_creation(data, p, d, q, P, D, Q, m, exog=None):\n",
    "    my_order = [p,d,q]\n",
    "    my_sorder = [P,D,Q,m]\n",
    "    sarimamod = sm.tsa.statespace.SARIMAX(data, exog, order=my_order, seasonal_order=my_sorder, \n",
    "                                          enforce_stationarity=False, enforce_invertibility=False,\n",
    "                                          initialization='approximate_diffuse')\n",
    "    model_fit = sarimamod.fit(disp=0)   # start_params=[0, 0, 0, 0, 1])\n",
    "    return(model_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T13:02:33.081089Z",
     "start_time": "2019-09-01T13:02:33.034217Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_creation_pred_one_step(train_data, test_data, exotrain=None, exotest=None, progress_bar=None):\n",
    "    ''' recursively makes forecast based on provided data for the next month\n",
    "        args: train_data = large data set to base predictions on\n",
    "              test_data  = decreasing dataset of data to test model\n",
    "              exotrain   = exogenous location data that matches the same timeframe of train_data but was not included\n",
    "              exotest    = exogenous location data that matches the same timeframe of test_data but was not included\n",
    "        returns: A list of all predictions for the location matching the entire test_data timeframe\n",
    "    '''\n",
    "    list_one_step = []\n",
    "    \n",
    "    nextMonth = model_based_forecast(train_data, exotrain)\n",
    "    list_one_step.append(nextMonth[0])             # captures prediction\n",
    "    progress_bar.update()\n",
    "\n",
    "    # if test data exists\n",
    "    if len(test_data) > 1:\n",
    "        # increment data for next month's iteration\n",
    "        train_data = pd.concat([train_data, test_data.iloc[[0]]])\n",
    "        test_data = test_data.drop(test_data.index[0], axis = 0)\n",
    "        if exotrain is not None:\n",
    "            exotrain = pd.concat([exotrain, exotest.iloc[[0]]])\n",
    "            exotest = exotest.drop(exotest.index[0], axis = 0)\n",
    "\n",
    "        # execute & capture future predictions\n",
    "        futurePredictions = model_creation_pred_one_step(train_data, test_data, exotrain, exotest, progress_bar)\n",
    "        # add to list\n",
    "        list_one_step.extend(futurePredictions)\n",
    "        \n",
    "    return(list_one_step)\n",
    "\n",
    "def model_based_forecast(train_data, exotrain=None):\n",
    "    ''' creates model from training data & makes a forecast\n",
    "        args: train_data = DataFrame to build forecasting model\n",
    "              exotrain   = DataFrame of exogenous location's rainfall data\n",
    "        returns: FLOAT value of next month's forecast value\n",
    "    '''\n",
    "    mod = sarima_model_creation(train_data, p=4, d=0, q=3, P=3, D=0, Q=4, m=12, exog=exotrain)\n",
    "    # if exists, passing exotrain's prevMonth (december, for forecasting jan), otherwise only forcast based on model\n",
    "    nextMonth = mod.forecast() if exotrain is None else mod.forecast(exog=exotrain.iloc[[-1]])       # turnary assignment expression\n",
    "    return(nextMonth)\n",
    "\n",
    "def maeFinder(train_data, test_data, exotrain=None, exotest=None, pbar=None):\n",
    "    ''' Function that finds the Mean Absolute Error between test_data and model-based predictions\n",
    "        args: train_data = large data set to base predictions on\n",
    "              test_data  = decreasing dataset of data to test model\n",
    "              exotrain   = exogenous location data that matches the same timeframe of train_data but was not included\n",
    "              exotest    = exogenous location data that matches the same timeframe of test_data but was not included\n",
    "              pbar       = Progress Bar object from tqdm, to provide updates to\n",
    "        returns: FLOAT of Mean Absolute Error value of potential exogenous location when included into model\n",
    "    '''\n",
    "    clone_train_data = copy.deepcopy(train_data)\n",
    "    clone_test_data = copy.deepcopy(test_data)\n",
    "    clone_exotrain = None if exotrain is None else copy.deepcopy(exotrain)\n",
    "    clone_exotest = None if exotest is None else copy.deepcopy(exotest)\n",
    "    \n",
    "    progressbar = pbar if pbar is not None else tqdm(total=len(test_data),leave=False) # initialize counter\n",
    "    \n",
    "    predictions = model_creation_pred_one_step(clone_train_data, clone_test_data, clone_exotrain, clone_exotest, progressbar)\n",
    "    if pbar is None:\n",
    "        progressbar.close()\n",
    "    \n",
    "    DECIMAL_PRECISION = 9\n",
    "    mae = round(mean_absolute_error(test_data, predictions), DECIMAL_PRECISION)\n",
    "    return(mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keymae(loc_name, data_targetloc, data_split_size):\n",
    "    ''' Function to evaluate the current location model.  It finds\n",
    "        the keymae of the current data frame about a location with a user defined percentage data split.\n",
    "        args: loc_name = Name of location to use as keyword in json\n",
    "              data_targetloc = dataframe of specific location and rainfall amounts over time\n",
    "              data_split_size = percentage in decimal form to define the data split to use to evaluate\n",
    "    '''\n",
    "    tr, test = train_test_split(data_targetloc, test_size=data_split_size, shuffle=False)\n",
    "    keymae = { 'loc_name': loc_name }\n",
    "    \n",
    "    shaObj = hashlib.sha1( bytes(data_targetloc.to_csv(), 'utf-8') )\n",
    "    data_signature = shaObj.hexdigest()\n",
    "    \n",
    "    try:\n",
    "        with open(results_filename, 'r') as all_results_file:\n",
    "            all_results = json.loads(all_results_file.read())\n",
    "    except:\n",
    "        all_results = {}\n",
    "    \n",
    "    if  loc_name in all_results and \\\n",
    "        'data_source_sha1' in all_results[loc_name] and \\\n",
    "        data_signature == all_results[loc_name]['data_source_sha1']:\n",
    "        \n",
    "         keymae['mae'] = all_results[loc_name]['keymae']\n",
    "    \n",
    "    else:\n",
    "        print(' ', end='', flush=True)   # weird hack that makes progress bars work in forked processes\n",
    "        with tqdm(desc=\"(keymae) \"+keymae['loc_name'],total=len(test),leave=False) as keymae_pbar:\n",
    "            keymae['mae'] = maeFinder(tr, test, pbar=keymae_pbar)\n",
    "        \n",
    "        # Save calculation to file\n",
    "        loc_data = {'keymae':keymae['mae'],'data_source_sha1': data_signature }\n",
    "        if 'exogen' in all_results[loc_name] and bool(all_results[loc_name]['exogen']):  # keep any previously processed exmaes\n",
    "            loc_data['exogen'] = all_results[loc_name]['exogen']\n",
    "        update_JSON_file(results_filename, None, (keymae['loc_name'], loc_data) ) # save with default adjuster\n",
    "        \n",
    "        # wipe bettermae file of any keymae results since keymae has been recalculated\n",
    "        def delete_location(data, location):\n",
    "            if location in data:\n",
    "                del data[location]\n",
    "            return(data)\n",
    "            \n",
    "        update_JSON_file(bettermae_results_filename, delete_location, (keymae['loc_name'],))\n",
    "        \n",
    "    return(keymae)\n",
    "\n",
    "def initKeymaeWorker(l):\n",
    "    ''' Constructor function for creating and establishing initial/global \n",
    "        variables across process pool.\n",
    "        args: l = synchronization lock object\n",
    "    '''\n",
    "    signal.signal(signal.SIGINT, signal.SIG_IGN)  # Turn off interrupt signal to child process\n",
    "    global lock\n",
    "    lock = l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_exmae(extr, extest):\n",
    "    ''' Standalone task method to find mae of a given exogenous variable.  \n",
    "        Intended to be used as the function for the process pool and handle memory synchronization\n",
    "        args: extr = exogenous location training data to be used for model training\n",
    "              extest = exogenous location test data to be evaluated against real data as a potential improvement model predictions\n",
    "        returns: Dictionary of exmae with columns\n",
    "        #bettermae state is saved to json file and updated synchronously across all forked processes\n",
    "    '''\n",
    "    co = tuple(extr.columns)\n",
    "    exog_name = '|'.join(co)\n",
    "    \n",
    "    shaObj = hashlib.sha1( bytes(pd.concat([extr,extest]).to_csv(), 'utf-8') )\n",
    "    data_signature = shaObj.hexdigest()\n",
    "    \n",
    "    # process syncrhonization on file read\n",
    "    lock.acquire()\n",
    "    try:\n",
    "        with open(results_filename, 'r') as all_results_file:\n",
    "            all_results = json.loads(all_results_file.read())  \n",
    "        if 'exogen' not in all_results[keymae['loc_name']]:\n",
    "            raise ValueError()  # first time run\n",
    "    except:\n",
    "        all_results = { keymae['loc_name']: {'exogen':{}} }\n",
    "    finally:\n",
    "        lock.release()\n",
    "    \n",
    "    exog_dict = all_results[keymae['loc_name']]['exogen']\n",
    "    if exog_name in exog_dict and \\\n",
    "       'data_source_sha1' in exog_dict[exog_name] and \\\n",
    "       data_signature == exog_dict[exog_name]['data_source_sha1']:\n",
    "        \n",
    "        exmae = exog_dict[exog_name]['exmae']\n",
    "        # Comment out the next line to rebuild allBetterMAE.json if needed\n",
    "        return { \"co\": co, \"exmae\": exmae }\n",
    "    \n",
    "    else:\n",
    "        print(' ', end='', flush=True)    # weird hack that makes progress bars work in forked processes\n",
    "        pbar_desc=\"(exmae)\"\n",
    "        with tqdm(desc=pbar_desc,total=len(test),leave=False) as exmae_pbar:\n",
    "            exmae = maeFinder(tr, test, extr, extest, exmae_pbar)\n",
    "    \n",
    "    def save_solved_exmae(all_solutions, targetloc, exogloc, exmae, data_hash):\n",
    "        ''' handler function for adjustment of JSON relating to results_file, \n",
    "            see adjustfn for update_JSON_file()\n",
    "            args: all_solutions = loaded python-equivalent of json from file\n",
    "                  targetloc = keyword of target location of current keymae\n",
    "                  exogloc = exogenous location name that improves the model\n",
    "                  exmae = value of mean absolute value\n",
    "                  data_hash = sha1 digest of exog data set used to calculate exmae\n",
    "            returns: dictionary object \n",
    "        '''\n",
    "        if 'exogen' not in all_solutions[targetloc]:\n",
    "            all_solutions[targetloc]['exogen'] = {}      # initialize exogen dictionary, when doesn't exist\n",
    "            \n",
    "        all_solutions[targetloc]['exogen'][exogloc] = { 'exmae': exmae, 'data_source_sha1': data_hash }\n",
    "        return(all_solutions)\n",
    "    \n",
    "    lock.acquire()\n",
    "    try:\n",
    "        # Update status file with solved exmae\n",
    "        update_JSON_file(results_filename, save_solved_exmae, (keymae['loc_name'], exog_name, exmae, data_signature))\n",
    "        \n",
    "        # Update bettermae array based on solved exmae if exmae is better than keymae\n",
    "        if exmae < keymae['mae']:\n",
    "            tmp_filename = tmp_bettermae_filename\n",
    "            update_JSON_file(tmp_filename, None, (exog_name, exmae))          # Save with default adjuster\n",
    "            \n",
    "    finally:\n",
    "        lock.release()\n",
    "        \n",
    "    return { \"co\": co, \"exmae\": exmae }\n",
    "\n",
    "\n",
    "def initExmaeWorker(l, targetloc_obj, list_exoloc):\n",
    "    ''' Constructor function for creating and establishing initial/global \n",
    "        variables across process pool.\n",
    "        args: l = synchronization lock object\n",
    "              targetloc_obj = {\n",
    "                  name = target location name keyword\n",
    "                  keymae = target location keymae value\n",
    "                  tr = training dataframe object from target location\n",
    "                  test = testing dataframe object from target location\n",
    "              }\n",
    "              list_exoloc = list of exogenous locations related to target location\n",
    "    '''\n",
    "    signal.signal(signal.SIGINT, signal.SIG_IGN)  # Turn off interrupt signal to child process\n",
    "    global lock\n",
    "    global keymae\n",
    "    global tr\n",
    "    global test\n",
    "    global l_exoloc\n",
    "    lock = l\n",
    "    keymae = { 'loc_name': targetloc_obj['name'], 'mae': targetloc_obj['keymae'] }\n",
    "    tr = targetloc_obj['tr']\n",
    "    test = targetloc_obj['test']\n",
    "    l_exoloc = list_exoloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targetloc_vars(data, targetlocations, data_split_size, progress_bars):\n",
    "    ''' Function to find all location's basic prediction model.\n",
    "        It spawns a pool of processes (# of CPU cores minus 1) to calculate each\n",
    "        mean absolute error of the model to the data.  Each keymae is printed to\n",
    "        stdout and stored into the results file.  The function does not complete\n",
    "        until all keymae values have been calculated.\n",
    "        args: \n",
    "              data : full rainfall DataFrame \n",
    "              targetlocations = list of all target location names\n",
    "              data_split_size = percentage of data set to test\n",
    "              progress_bars = {\n",
    "                  'keymae_pbar' = tqdm object for keymae program progress \n",
    "                  'total_pbar' = tqdm object for entire program progress\n",
    "              }\n",
    "    '''\n",
    "    # unpack progress_bars\n",
    "    total_progress = progress_bars['total_pbar']\n",
    "    keymae_progress = progress_bars['keymae_pbar']\n",
    "    \n",
    "    # process keymaes\n",
    "    poolLock = multiprocessing.Lock()\n",
    "    process_limit = multiprocessing.cpu_count()-1          # 1 cpu is needed for basic OS functions\n",
    "    \n",
    "    def on_success(result):\n",
    "        print('keymae of {0} = {1}'.format(result['loc_name'],str(result['mae'])), flush=True)\n",
    "        # update counter of completion\n",
    "        keymae_progress.update()\n",
    "        total_progress.update(math.ceil(data.shape[0]*data_split_size))\n",
    "    \n",
    "    def on_error(err):\n",
    "        print(\"ERROR: {}\".format(err), flush=True)\n",
    "        traceback.print_exception(type(err), err, err.__traceback__) \n",
    "\n",
    "\n",
    "    # create pool processes & set global/shared variables\n",
    "    pool = multiprocessing.Pool(\n",
    "        processes=process_limit, \n",
    "        initializer=initKeymaeWorker, \n",
    "        initargs=(poolLock,)\n",
    "    )\n",
    "    print(\"[Exogenous_Variables] Created keymae processing pool with {} processes\".format(pool._processes), flush=True)\n",
    "    \n",
    "    for loc in targetlocations:\n",
    "        pool.apply_async(\n",
    "            find_keymae, \n",
    "            args=(loc, data[loc], data_split_size), \n",
    "            kwds={}, \n",
    "            callback=on_success, \n",
    "            error_callback=on_error\n",
    "        )\n",
    "    \n",
    "    pool.close()      # no more tasks can be added for the pool to accomplish\n",
    "    print(\"[Exogenous_Variables] Pool executing {0} tasks across {1} processes\".format(len(targetlocations), pool._processes), flush=True)\n",
    "    try:\n",
    "        print(\"[Exogenous_Variables] waiting for keymae workers...\", flush=True)\n",
    "        pool.join()       # tell parent to wait until all tasks are accomplished by the process pool\n",
    "    except KeyboardInterrupt:\n",
    "        print (\"\\nMultiprocessing Pool: KeyboardInterrupt. Terminating keymae workers...\")\n",
    "        pool.terminate()\n",
    "        pool.join()\n",
    "        raise KeyboardInterrupt() # continue bubble up to kill parent process\n",
    "\n",
    "    # Collect all Keymae values that were found\n",
    "    if os.path.isfile(results_filename):\n",
    "        with open(results_filename, 'r') as all_results_file:\n",
    "            all_results = json.loads(all_results_file.read())\n",
    "\n",
    "        keymae_storage = {}                                          # initialize storage dictionary\n",
    "        for loc_name,result in all_results.items():                   # extract keymae values only\n",
    "            keymae_storage[loc_name] = result['keymae']\n",
    "            \n",
    "        return(keymae_storage)\n",
    "    else:    \n",
    "        raise FileNotFoundError(\"Missing file {}\".format(results_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exogenous_var(obj_targetloc, data_split_size, l_exoloc, progress_bars):\n",
    "    ''' Function to evaluate a location's exogenous variables and their affect on the prediction model\n",
    "        It spawns a pool of processes (# of CPU cores minus 1) to calculate each potential\n",
    "        exogenous location's potential improvement of the model.  Each exmae is printed to\n",
    "        stdout and if improved, it is stored into the bettermae dictionary.  The target location\n",
    "        does not complete until all exmaes have been calculated.\n",
    "        args: \n",
    "              obj_targetloc = {\n",
    "                 'data' : DataFrame\n",
    "                 'name' : location name\n",
    "                 'keymae' : value\n",
    "              }\n",
    "              data_split_size = percentage in decimal form to define the data split to use to evaluate\n",
    "              l_exoloc = list of exogenous locations to the ncloc parameter\n",
    "              progress_bars = {\n",
    "                  'exmae_pbar' = tqdm object for exmae program progress \n",
    "                  'total_pbar' = tqdm object for entire program progress\n",
    "              }\n",
    "    '''\n",
    "    # unpack progress_bars\n",
    "    total_progress = progress_bars['total_pbar']\n",
    "    exmae_progress = progress_bars['exmae_pbar']\n",
    "    \n",
    "    # process exmaes\n",
    "    poolLock = multiprocessing.Lock()\n",
    "    process_limit = multiprocessing.cpu_count()-1          # 1 cpu is needed for basic OS functions\n",
    "    progressbar = tqdm(total=len(l_exoloc),leave=False)                # initialize counter (regular)\n",
    "    \n",
    "    def on_success(result):\n",
    "        print('exmae = {}'.format(result[\"co\"]) + ' '+ str(result[\"exmae\"]), flush=True)\n",
    "        # update counter(s) of completion\n",
    "        progressbar.update()\n",
    "        exmae_progress.update()\n",
    "        total_progress.update(math.ceil(obj_targetloc['data'].shape[0]*data_test_percentage))\n",
    "    \n",
    "    def on_error(err):\n",
    "        print(\"ERROR: {}\".format(err), flush=True)\n",
    "        traceback.print_exception(type(err), err, err.__traceback__)\n",
    "\n",
    "    # same for every exmae\n",
    "    obj_targetloc['tr'], obj_targetloc['test'] = train_test_split(obj_targetloc['data'], test_size=data_split_size, shuffle=False)\n",
    "    # create pool processes & set global/shared variables\n",
    "    pool = multiprocessing.Pool(\n",
    "        processes=process_limit, \n",
    "        initializer=initExmaeWorker, \n",
    "        initargs=(poolLock, obj_targetloc, l_exoloc)\n",
    "    )\n",
    "    print(\"[Exogenous_Variables] Created exmae processing pool with {} processes\".format(pool._processes), flush=True)\n",
    "        \n",
    "    for exog in l_exoloc:\n",
    "        extr, extest = train_test_split(exog, test_size=data_split_size, shuffle=False)\n",
    "        pool.apply_async(find_exmae, args=(extr,extest), kwds={}, callback=on_success, error_callback=on_error)\n",
    "    \n",
    "    pool.close()      # no more tasks can be added for the pool to accomplish\n",
    "    print(\"[Exogenous_Variables] Pool executing {0} tasks across {1} processes\".format(len(l_exoloc), pool._processes), flush=True)\n",
    "        \n",
    "    try:\n",
    "        print(\"[Exogenous_Variables] waiting for exmae workers...\", flush=True)\n",
    "        pool.join()       # tell parent to wait until all tasks are accomplished by the process pool\n",
    "    except KeyboardInterrupt:\n",
    "        print (\"\\nMultiprocessing Pool: KeyboardInterrupt. Terminating exmae workers...\")\n",
    "        pool.terminate()\n",
    "        pool.join()\n",
    "        raise KeyboardInterrupt() # continue bubble up to kill parent process\n",
    "    finally:\n",
    "        progressbar.close()   # End progress bar for the entire pool of exmaes\n",
    "    \n",
    "    # Evaluate & save found bettermae\n",
    "    if os.path.isfile(tmp_bettermae_filename):\n",
    "        tmp_bettermae_file = open(tmp_bettermae_filename, 'r')\n",
    "        improvement_exog = json.loads(tmp_bettermae_file.read())\n",
    "        tmp_bettermae_file.close()\n",
    "        os.remove(tmp_bettermae_file.name)                               # tmp file cleanup\n",
    "        \n",
    "        all_results_file = open(results_filename, 'r')\n",
    "        all_results = json.loads(all_results_file.read())\n",
    "        all_results_file.close()\n",
    "            \n",
    "        filtered_results = all_results[obj_targetloc['name']]\n",
    "        filtered_results['exogen'] = {}                              # reset dictionary\n",
    "        for key,value in improvement_exog.items():                   # fill exogen dictionary with valuable vars\n",
    "            filtered_results['exogen'][key] = value\n",
    "            \n",
    "        all_bettermae = update_JSON_file(bettermae_results_filename, None, (obj_targetloc['name'], filtered_results)) # save with default adjuster\n",
    "        print(\"Improvement_exog: {0}: {1}\".format(obj_targetloc['name'], json.dumps(all_bettermae[obj_targetloc['name']], indent=4)))\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_JSON_file(filename, adjustfn, arglist=(), kwargs={}, sort=True):\n",
    "    ''' Generic function to handle JSON file updates.  Reads-in entire file, \n",
    "        federates out updates with adjustment fn's, and then overwrites original file completely\n",
    "        Handles FileNotFoundError & JSONDecodeError automatically.\n",
    "        args: filename = json-encoded file on disk\n",
    "              adjustfn = function to perform adjustments to loaded dictionary file\n",
    "              arglist = positional args to pass on to adjustfn\n",
    "              kwargs = keyword args to pass on to adjustfn\n",
    "              sort = flag to auto-sort keys when saving to file [Default = True]\n",
    "        returns: dictionary object that was updated and saved to file\n",
    "    '''\n",
    "    def default_dict_adjustfn(data, key, value):\n",
    "        ''' Generic default function for updating a basic dictionary data file (top level keys only)\n",
    "            args: data = dictionary representation of JSON data from file\n",
    "                  key = key name to enter into dictionary\n",
    "                  value = value to enter into dictionary[key]\n",
    "            returns: Updated dictionary with key/value added\n",
    "        '''\n",
    "        data[key] = value\n",
    "        return(data)\n",
    "    \n",
    "    def default_list_adjustfn(data, value):\n",
    "        ''' Generic default function for updating a basic list data file (add to bottom of list)\n",
    "            args: data = list representation of JSON data from file\n",
    "                  value = value to append to end of list, list[len(list)] = value\n",
    "            returns: Updated list with value appended\n",
    "        '''\n",
    "        data.append(value)\n",
    "        return(data)\n",
    "    \n",
    "    loaded = False\n",
    "    while not loaded:\n",
    "        try:\n",
    "            file = open(filename, \"r+\")\n",
    "            json_data = json.loads(file.read())\n",
    "        except FileNotFoundError:\n",
    "            open(filename, \"w+\").close()       # create file on disk\n",
    "            continue\n",
    "        except json.JSONDecodeError:\n",
    "            json_data = {}\n",
    "        \n",
    "        loaded = True\n",
    "        if adjustfn is not None:\n",
    "            json_data = adjustfn(json_data, *arglist, **kwargs)\n",
    "        else:\n",
    "            if isinstance(json_data, dict):\n",
    "                json_data = default_dict_adjustfn(json_data, *arglist, **kwargs)\n",
    "            elif isinstance(json_data, list):\n",
    "                json_data = default_list_adjustfn(json_data, *arglist, **kwargs)\n",
    "            else:\n",
    "                raise ValueError('Unable to adjust JSON since function not provided or file not of type dict or list!')\n",
    "        \n",
    "        file.seek(0)                           # Go to first line, first column of file\n",
    "        file.write( json.dumps(json_data, sort_keys=sort, indent=4)+'\\n')\n",
    "        file.truncate()                        # end file here, delete anything after the current file position\n",
    "        file.close()\n",
    "    \n",
    "    return(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T20:23:59.034914Z",
     "start_time": "2019-07-31T20:23:59.003666Z"
    }
   },
   "outputs": [],
   "source": [
    "def exog_combinations(df, exoe):\n",
    "    ''' This function takes the dataframe of rain data and the list of exogenous variables from a single NC\n",
    "    location and then returns a list of dataframes that contains all of the rainfall data for just the \n",
    "    exogenous variables\n",
    "    '''\n",
    "    lo_dfs = []\n",
    "    if len(exoe) == 1:\n",
    "        lo_dfs.append(df.loc[:,exoe])\n",
    "    if len(exoe) > 1:\n",
    "        lo_dfs.append(df.loc[:,exoe])\n",
    "        for ex in exoe:\n",
    "            lo_dfs.append(df.loc[:,[ex]])\n",
    "        if len(exoe) >2:\n",
    "            for i in range(2, len(exoe)):\n",
    "                combolist = list(combinations(exoe,i))\n",
    "                for c in combolist:\n",
    "                    lo_dfs.append(df.loc[:,c])\n",
    "    return(lo_dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Evaluation\n",
    "Finds combinations of exogenous variable locations and starts model evaluation of combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoExogen = True   # flag for manual use\n",
    "\n",
    "# Defining set of cities to evaluate\n",
    "if autoExogen or getpass.getuser() == \"rainfalld\":       # docker daemon, automatically do all exogen\n",
    "    todokeys = exogen.keys()\n",
    "else:    # manual setting of dictionary elements to do\n",
    "    todokeys = ('ARCOLA, NC', 'HENDERSON 2 NNW, NC', 'LAURINBURG, NC', 'ROANOKE RAPIDS, NC', 'MURFREESBORO, NC', 'LUMBERTON AREA, NC', 'LONGWOOD, NC', 'WHITEVILLE 7 NW, NC', 'CHARLOTTE AREA, NC', 'MOUNT MITCHELL AREA, NC', 'ASHEVILLE AIRPORT, NC', 'BANNER ELK, NC', 'BEECH MOUNTAIN, NC', 'BRYSON CITY 4, NC', 'BREVARD, NC', 'CASAR, NC', 'COWEETA EXP STATION, NC', 'CULLOWHEE, NC', 'FOREST CITY 8 W, NC', 'FRANKLIN, NC', 'GASTONIA, NC', 'GRANDFATHER MTN, NC', 'HENDERSONVILLE 1 NE, NC', 'HIGHLANDS, NC', 'HOT SPRINGS, NC', 'LAKE LURE 2, NC', 'LAKE TOXAWAY 2 SW, NC', 'MARSHALL, NC', 'MONROE 2 SE, NC', 'MOUNT HOLLY 4 NE, NC', 'OCONALUFTEE, NC', 'PISGAH FOREST 3 NE, NC', 'ROBBINSVILLE AG 5 NE, NC', 'ROSMAN, NC', 'SHELBY 2 NW, NC', 'TAPOCO, NC', 'TRYON, NC', 'WAYNESVILLE 1 E, NC', 'BOONE 1 SE, NC', 'DANBURY, NC', 'EDEN, NC', 'MOUNT AIRY 2 W, NC', 'REIDSVILLE 2 NW, NC', 'HAYESVILLE 1 NE, NC', 'MURPHY 4ESE, NC', 'KING, NC')\n",
    "\n",
    "sub_exogen = {k: exogen[k] for k in todokeys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T20:24:01.912170Z",
     "start_time": "2019-07-31T20:24:01.380989Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "l_o_dfs = defaultdict(list)\n",
    "for key,value in tqdm(sub_exogen.items()):\n",
    "    lo_dfs2 = exog_combinations(rd, value)\n",
    "    l_o_dfs[key] = lo_dfs2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_filename = os.path.join(destdir,\"allMAE.json\")\n",
    "bettermae_results_filename = os.path.join(destdir,\"allBetterMAE.json\")\n",
    "tmp_bettermae_filename = os.path.join(destdir, \"tmp_bettermae.json\")\n",
    "\n",
    "# best_comb = [[4,3,3,4]]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "files=[tmp_bettermae_filename]\n",
    "while (len(files) > 0):                          # reset results on new run\n",
    "    try:\n",
    "        os.remove( files[-1] )\n",
    "    except FileNotFoundError:                    # ignore since non-exist is the desired state\n",
    "        pass\n",
    "    except OSError as err:\n",
    "        traceback.print_exception(type(err), err, err.__traceback__)\n",
    "    finally:\n",
    "        files.pop()\n",
    "\n",
    "\n",
    "data_test_percentage = 0.2                                           # 20%\n",
    "num_single_predictions = math.ceil(rd.shape[0]*data_test_percentage)\n",
    "num_all_predictions = len(l_o_dfs.items())*num_single_predictions    # keymae predictions amount\n",
    "num_all_exmae = 0\n",
    "for key,value in l_o_dfs.items():\n",
    "    num_all_exmae += len(value)                                      # exmae predictions amount\n",
    "num_all_predictions += num_all_exmae*num_single_predictions\n",
    "\n",
    "keymae_storage = {}    \n",
    "# tqdmformat = '{desc}: |{bar}|{percentage:3.0f}%'\n",
    "total_progress = tqdm(desc=\"Full Calculation:\", total=num_all_predictions, position=0)\n",
    "keymae_progress = tqdm(desc=\"Finding keymaes:\", total=len(l_o_dfs.items()), position=1)\n",
    "exmae_progress = tqdm(desc=\"Evaluating exmaes:\", total=num_all_exmae, position=2)\n",
    "\n",
    "try:\n",
    "    # Solve for targetloc Keymae Values first\n",
    "    print(\"============== KEYMAE EVALUATION ===============\")\n",
    "    pbars = { 'total_pbar': total_progress, 'keymae_pbar': keymae_progress, 'exmae_pbar': exmae_progress }\n",
    "    keymae_storage = targetloc_vars(rd, list(l_o_dfs.keys()), data_test_percentage, pbars)\n",
    "\n",
    "    # Solve for exmae values of each combination of targetloc and matching exogenous variable\n",
    "    print(\"\\n=============== EXMAE EVALUATION ===============\")\n",
    "    for key,value in l_o_dfs.items():\n",
    "        print(\"\\nFinding exmaes values for {0}:\".format(key))\n",
    "        targetloc_obj = { 'data': rd[key], 'name': key, 'keymae': keymae_storage[key] }\n",
    "        exogenous_var(targetloc_obj, data_test_percentage, value, pbars)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"MANUAL EXIT: Program interrupted by user.\", flush=True)\n",
    "    raise SystemExit(2)\n",
    "except Exception as err:\n",
    "    print(\"ERROR: {}\".format(err), flush=True)\n",
    "    traceback.print_exception(type(err), err, err.__traceback__)\n",
    "    raise SystemExit(1)\n",
    "else:\n",
    "    print(\"\\n==== EXOGENOUS VARIABLE EVALUATION COMPLETE ====\\n\")\n",
    "finally:\n",
    "    keymae_progress.close()\n",
    "    exmae_progress.close()\n",
    "    total_progress.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
