{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data on the website are not in one place, but spread out through several pages on the \n",
    "Southeast regional Climate Center website. Thus, my first step was not within python but gathering\n",
    "the data and placing it into a manageable format.\n",
    "\n",
    "North Carolina is separated into 3 regions Morehead City, Raleigh/Durham, and Wilmington. However, all of the weather stations that the SRCC uses in North Carolina are not all contained in these 3 regions. Several other regions that are placed under other states have regions that overlap into North Carolina, thus one must click on each of the regions within NC and all of the neighboring states regions in order to gather the data for all of the stations that are located in North Carolina. Then I had to go through and click on each location within the region, select the appropriate monthly rain data and copy and paste this data into an excel file as an individual sheet.\n",
    "\n",
    "In addition, my excel data file contains surrounding locations from these neighboring states that are close to North Carolina. We will use these points as exogenous datapoints to see any relationship between these locations and the locations within NC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "\n",
    "I imported the necessary modules and then load the excel spreadsheet that I used to collect all of the data from the website.  I set the destination directory for where the manipulated data will be stored at the end of the Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T14:53:29.340770Z",
     "start_time": "2019-09-08T14:52:56.092135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Output Files Exist & Data Integrity checked!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "import csv\n",
    "import json\n",
    "from shasum import shasum\n",
    "\n",
    "# Dynamic filepath finding\n",
    "try: \n",
    "    __file__    # Jupyter vs regular python detection\n",
    "except:\n",
    "    curr_dir = os.path.abspath('')\n",
    "else:\n",
    "    curr_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    \n",
    "app_root = curr_dir if os.path.basename(curr_dir) != \"src\" else os.path.dirname(curr_dir)\n",
    "\n",
    "if getpass.getuser() == \"rainfalld\":  # docker daemon\n",
    "    home = os.path.expanduser(\"~\")\n",
    "    destdir = home                    # /var/cache/rainfall-predictor\n",
    "else:\n",
    "    destdir = os.path.join(app_root,'data','manipulated_data')      # non-docker stay in repository\n",
    "\n",
    "# Load Files\n",
    "file = os.path.join(app_root,'data','raw_data','NC Monthly Precipitation Data.xlsx')\n",
    "latlong = pd.read_csv(os.path.join(app_root,'data','raw_data','latlong.csv'))\n",
    "NCdata = pd.ExcelFile(file)\n",
    "\n",
    "\n",
    "# Check if Data_Wrangling has already been accomplished\n",
    "# If so, exit as success! If not or if changed dataset, accomplish\n",
    "if getpass.getuser() == \"rainfalld\":                                # docker daemon\n",
    "    original_datasets = { \n",
    "        \"NC Monthly Precipitation Data.xlsx\": shasum().file_sha(file),\n",
    "        \"latlong.csv\": shasum().sha(latlong.to_csv())\n",
    "    }\n",
    "    output_data = {\n",
    "        os.path.join(destdir,'rainfalldata.csv') : None,\n",
    "        os.path.join(destdir,'distances.csv') : None,\n",
    "        os.path.join(destdir,'ncrainfalldata.csv') : None,\n",
    "        os.path.join(destdir,'exogen.json') : None\n",
    "    }\n",
    "    \n",
    "    i = 0\n",
    "    output_files = list(output_data.keys())\n",
    "    try:\n",
    "        while i < len(output_files):\n",
    "            output_filename = output_files[i]\n",
    "            output_data[output_filename] = shasum().file_sha(output_filename)\n",
    "            i += 1\n",
    "\n",
    "    except FileNotFoundError: \n",
    "        if i == 0:\n",
    "            print(\"[DATA_WRANGLING] No previous data wrangling output found.  Starting from scratch!\", flush=True)\n",
    "        else:\n",
    "            print(\"[DATA_WRANGLING] FileNotFoundError for file '{}'\".format(output_files[i]))\n",
    "            print(\"[DATA_WRANGLING] Handling missing file by starting from scratch!\", flush=True)\n",
    "\n",
    "    except:            # any error will cause entire Data Wrangling to run normally\n",
    "        print(\"[DATA_WRANGLING] starting from scratch!\", flush=True)\n",
    "\n",
    "    else:\n",
    "        completion_signature = shasum().sha( json.dumps({ **original_datasets, **output_data }) )\n",
    "#         print(completion_signature)      # to update: uncomment this line, enable if-statement, run, and replace the next line's value\n",
    "        if completion_signature == \"7c8b32620e984fe4450e1f97fe75ad9361ec59dfc0a95d4d8cf8492baf76da46\":\n",
    "            print(\"COMPLETE: Output Files Exist & Data Integrity checked!\")\n",
    "            raise SystemExit(0)  # No need to complete rest of file\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "I needed to bring all of the separate sheets in excel together into a single dataframe. Thus, this function uses a for loop to parse out each sheet from the excel file. Once I had the sheet, all of the months were in separate columns. The function takes all of the months and places them next to the year then places the rainfall amounts for each month in the next column. The function merges the resulting dataframe into the blank dataframe on the year and the month columns with an outer join in order to catch all the data from both the original and merging dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T14:53:38.768805Z",
     "start_time": "2019-09-08T14:53:29.710650Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blank = pd.DataFrame()\n",
    "def datachunks(s, e, df): #s and e stand for the beginning and end of the chunk you want\n",
    "    for i in range(s,e):\n",
    "        to_merge = NCdata.parse(i, skiprows=[0,1], usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12]) #first two rows in the data were titles\n",
    "        to_merge = to_merge.dropna() #removes two rows from the data that were labeled as NaN and not needed\n",
    "        to_merge = to_merge.set_index('Year') #set the index to year to remove the following 3 rows\n",
    "        to_merge = to_merge.drop(['Mean','Max', 'Min'])\n",
    "        to_merge = to_merge.reset_index() #resets the index so that the dataframe can be melted on Year\n",
    "        to_merge1 = pd.melt(to_merge, id_vars=['Year'], value_vars=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug',\n",
    "        'Sep','Oct','Nov','Dec'], var_name='Month') \n",
    "        to_merge1.iloc[:,2] = pd.to_numeric(to_merge1.iloc[:,2], errors = 'coerce')\n",
    "        if i == 0:\n",
    "            df = to_merge1\n",
    "        else:\n",
    "            df = pd.merge(df, to_merge1, on = ['Year','Month'], how = 'outer')\n",
    "    return df\n",
    "\n",
    "ncdata = datachunks(0,234, blank)\n",
    "ncdata.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "Column names to be placed on top of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T14:58:51.387774Z",
     "start_time": "2019-09-08T14:58:51.372148Z"
    }
   },
   "outputs": [],
   "source": [
    "colnames = ['YEAR', 'MONTH']\n",
    "names = NCdata.sheet_names\n",
    "for idx, name in enumerate(names):\n",
    "    names[idx] = name.upper().strip()\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T14:58:53.092121Z",
     "start_time": "2019-09-08T14:58:53.045248Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ncdata.columns = colnames + names\n",
    "ncdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "In order to sort the data based on the year and month I needed to first convert the columns to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T14:59:50.793412Z",
     "start_time": "2019-09-08T14:59:50.605921Z"
    }
   },
   "outputs": [],
   "source": [
    "ncdata[\"MONTH\"] = pd.to_datetime(ncdata.MONTH, format='%b', errors='coerce').dt.month\n",
    "ncdata[\"YEAR\"] = pd.to_datetime(ncdata.YEAR, format='%Y', errors='coerce').dt.year\n",
    "ncdata[['YEAR', 'MONTH']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:00:02.515318Z",
     "start_time": "2019-09-08T15:00:02.296580Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this sorts the data based on year then month\n",
    "ncdata_sorted = ncdata.sort_values(['YEAR','MONTH'])\n",
    "ncdata_sorted.tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5\n",
    "Instead of separate columns for year and month the following code creates a single Date column and sets it as the index. The index is a string because datetime does not allow for dates without a day; however, having a day listed in the datetime would not be reasonable in this dataset because these are monthly totals of rainfall not occurring on a single day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:00:40.713908Z",
     "start_time": "2019-09-08T15:00:39.973636Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ncdata_sorted['YEAR'] = ncdata_sorted.YEAR.apply(str)\n",
    "ncdata_sorted['MONTH'] = ncdata_sorted.MONTH.apply(str)\n",
    "ncdata_sorted['Date'] = ncdata_sorted['MONTH'] + '-' + ncdata_sorted['YEAR']\n",
    "ncdata1 = ncdata_sorted.set_index('Date')\n",
    "ncdata1 = ncdata1.drop(['YEAR', 'MONTH'], axis=1)\n",
    "ncdata1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6 - removing impossible data\n",
    "I gathered this data in May 2019; thus, it was impossible to have any totals from months that had not happened yet; therefore, I removed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:00:49.178974Z",
     "start_time": "2019-09-08T15:00:49.085279Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ncdata1 = ncdata1.drop(['5-2019', '6-2019', '7-2019', '8-2019','9-2019','10-2019','11-2019','12-2019'], axis=0)\n",
    "ncdata1.tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7\n",
    "There was a lot of missing data from several locations. Due to this I created the following for loop in order to see which rows (corresponding to a single month) had at least 70% of data. Since the function len() counts missing data while the method .count() does not, I used these two functions to figure out the percentage that each row has and made it a column in the dataframe called 'percent_number'\n",
    "\n",
    "I found that from January 1956-present all had data from at least 70% of the locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:01:13.217122Z",
     "start_time": "2019-09-08T15:01:12.357776Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lop = []\n",
    "for i in ncdata1.index:\n",
    "    l = len(ncdata1.loc[i])\n",
    "    c = ncdata1.loc[i].count()\n",
    "    percent = (c/l)*100\n",
    "    if i == 0:\n",
    "        lop = [percent]\n",
    "    else:\n",
    "        lop = lop + [percent]\n",
    "ncdata1['percent_number'] = lop\n",
    "ncdata1.percent_number[ncdata1.percent_number >= 80].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8\n",
    "Since I had the Date column as my index and I didn't want to remove it, I created a new index row called row_number. I used the row number to figure out which row 1-1956 was located at in order to create the dataframe that includes only data from 1-1956-present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:01:18.824298Z",
     "start_time": "2019-09-08T15:01:18.793053Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nl = [i for i in range(1948)]\n",
    "ncdata1['row_number'] = nl \n",
    "ncdata1.row_number.loc['1-1980'] # provides the row which Jan 1956 is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:01:21.534094Z",
     "start_time": "2019-09-08T15:01:21.456023Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ncdata_80 = ncdata1[ncdata1.row_number >= 1476]\n",
    "ncdata_80.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:01:26.674506Z",
     "start_time": "2019-09-08T15:01:26.658879Z"
    }
   },
   "outputs": [],
   "source": [
    "#this drops the percent number column that is no longer needed.\n",
    "ncdata_80 = ncdata_80.drop(['percent_number'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9\n",
    "The dataset also has some missing data from when the location began gathering data to the present. For example, even though Raleigh, NC has been gathering data from 1-1956 until the present there was one month in October of 2000 where the monthly total was not recorded. Thus the following function fills in missing data that are contained within the locations. This function finds the months with missing data and fills them in by averaging the rainfall totals from the previous year, previous month and next month. If the any of these points are not available either the function uses the data from two years prior, two months prior, or two months after. If just one of these is not available it ignores the NaN and averages the other two, otherwise it keeps the datapoint as NaN. Thus, this function does not get rid of all missing values, but fills in the missing values as long as there is adequate data to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:01:30.947807Z",
     "start_time": "2019-09-08T15:01:30.900956Z"
    }
   },
   "outputs": [],
   "source": [
    "def missingfill(df, column):\n",
    "    missing = df.index[df[column].isnull()]\n",
    "    if len(missing) > 0:\n",
    "        for n in missing:\n",
    "            moth = df.loc[n].row_number # finds the row index for the missing data point\n",
    "            if ((moth >= 1488) & (moth <= 1945)): #must be a year after 1-1956 (rownumber=1188) otherwise it is impossible to have the previous year's data to gather from\n",
    "                ly = moth - 12 #the previous year's row number\n",
    "                lyrd = df[[column]][df['row_number'] == ly] # the previous year's rainfall amount as a dataframe\n",
    "                lyrd1 = lyrd[column][0] #separates the value of the previous year's dataframe to just the rainfall amount\n",
    "                lm = moth - 1 # the next 6 lines perform the same as the previous 3 except for previous month and following month\n",
    "                lmrd = df[[column]][df['row_number'] == lm]\n",
    "                lmrd1 = lmrd[column][0]\n",
    "                nm = moth + 1\n",
    "                nmrd = df[[column]][df['row_number'] == nm]\n",
    "                nmrd1 = nmrd[column][0]\n",
    "                if ((math.isnan(lyrd1)) & (moth >= 1500)): # if the previous year was not available, go back 2 years\n",
    "                    twy = moth - 24\n",
    "                    twyrd = df[[column]][df['row_number'] == twy]\n",
    "                    lyrd1 = twyrd[column][0]\n",
    "                if (math.isnan(lmrd1)): #if the previous month was not available, go back 2 months\n",
    "                    lm = moth - 2\n",
    "                    lmrd = df[[column]][df['row_number'] == lm]\n",
    "                    lmrd1 = lmrd[column][0]\n",
    "                if (math.isnan(nmrd1)): #if the next month was not available, go forward 2 months\n",
    "                    nm = moth + 2\n",
    "                    nmrd = df[[column]][df['row_number'] == nm]\n",
    "                    nmrd1 = nmrd[column][0]\n",
    "                newpoint = np.nanmean([lyrd1,lmrd1,nmrd1]) #finds the average of the 3 values \n",
    "                df.loc[n,column] = newpoint #places the value into the missing data slot\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10 \n",
    "performs the function defined in the previous cell and applies the function to every column. The runtime warning just means that np.nanmean has only nan values and thus is returning a nan value again, which is fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:05:11.828046Z",
     "start_time": "2019-09-08T15:01:34.549197Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in ncdata_80.columns:\n",
    "    ncdata_80 = missingfill(ncdata_80, i)\n",
    "ncdata_80.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 11\n",
    "\n",
    "Drop the row_number column since it is no longer needed, and make a csv file from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:05:12.138371Z",
     "start_time": "2019-09-08T15:05:12.045913Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drops the row_number column since it is no longer needed. \n",
    "ncdata_80 = ncdata_80.drop(['row_number'],axis=1)\n",
    "ncdatarein = ncdata_80.reset_index()\n",
    "# #converts Date to datetime object\n",
    "ncdatarein['Date'] = pd.to_datetime(ncdatarein['Date'])\n",
    "alldatadf = ncdatarein.set_index('Date')\n",
    "alldatadf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 12\n",
    "\n",
    "Removal of duplicate sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:07:51.219481Z",
     "start_time": "2019-09-08T15:07:51.209487Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alldatadf = alldatadf.drop(['RALEIGH AP, NC', 'GREENSBORO, NC', 'WILMINGTON 7 N, NC','LUMBERTON, NC',\n",
    "                            'MYRTLE BEACH, SC','CHARLOTTE DOUGLAS AIRPORT, NC','GRNVL SPART INTL AP, SC',\n",
    "                            'PICKENS, SC','MT. MITCHELL, NC','CAESARS HEAD AREA, SC'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is still the issue of missing data for some sites. The next steps fill in that missing data using locations that are within 85 kilometers of the location with missing data. First the latitude and longitude coordinates of each site are used to calculate the distance between sites. Then a function is used to subset only the surrounding locations of the location with missing data. Lastly, if there is more than one external location within 85 kilometers to the location with missing data then the average of those external locations were taken. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 13\n",
    "\n",
    "Providing distance data from all locations to all other locations using LAT/LONG coordinates. First, the haversine function calculates the distance between two points on the globe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:08:04.077711Z",
     "start_time": "2019-09-08T15:08:04.066715Z"
    }
   },
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    ''' uses latitude and longitude coordinates of two locations and computes the distance in kilometers\n",
    "    '''\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])    \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1    \n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    return 2 * 6371 * asin(sqrt(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 14\n",
    "\n",
    "Importing the latitude and longitude for each location that is in a csv file. Then separating the latitude and longitude for each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:08:07.720377Z",
     "start_time": "2019-09-08T15:08:07.715383Z"
    }
   },
   "outputs": [],
   "source": [
    "latlongsplit = latlong.iloc[0].apply(str.split, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 15\n",
    "\n",
    "Creating the file as a pandas dataframe and then removing all duplicate locations as completed in step 12 for the filled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:17:51.302369Z",
     "start_time": "2019-09-08T15:17:51.255492Z"
    }
   },
   "outputs": [],
   "source": [
    "latlongdf = pd.DataFrame(latlongsplit)\n",
    "latlongdf = latlongdf.drop(['Unnamed: 0'])\n",
    "latlongdf_index = list(latlongdf.index)\n",
    "for idx, name in enumerate(latlongdf_index):\n",
    "    latlongdf_index[idx] = name.upper().strip()\n",
    "latlongdf.index = latlongdf_index\n",
    "latlongdf = latlongdf.drop(['RALEIGH AP, NC', 'GREENSBORO, NC', 'WILMINGTON 7 N, NC','LUMBERTON, NC',\n",
    "                            'MYRTLE BEACH, SC','CHARLOTTE DOUGLAS AIRPORT, NC','GRNVL SPART INTL AP, SC',\n",
    "                            'PICKENS, SC','MT. MITCHELL, NC','CAESARS HEAD AREA, SC'])\n",
    "latlongdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 16 \n",
    "\n",
    "Next step is to find the distance between all of the locations which is done using the function below. It outputs a dataframe with the calculated distance between all locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:18:12.455968Z",
     "start_time": "2019-09-08T15:18:12.424743Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distance_loc(df):\n",
    "    '''calculates the distance between all locations and places it into a dataframe\n",
    "    '''\n",
    "    didf = pd.DataFrame(columns=df.index)\n",
    "    row = {}\n",
    "    for index in df.index:\n",
    "        lat1, long1 = float(df.loc[index][0][0]), float(df.loc[index][0][1])\n",
    "        for i in df.index:\n",
    "            lat2, long2 = float(df.loc[i][0][0]), float(df.loc[i][0][1])\n",
    "            dist = haversine(long1, lat1, long2, lat2)\n",
    "            row[i] = dist\n",
    "        didf = didf.append(row, ignore_index=True)\n",
    "    return(didf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:18:44.242948Z",
     "start_time": "2019-09-08T15:18:21.048452Z"
    }
   },
   "outputs": [],
   "source": [
    "# index is the same as the columns so just names the columns the same as the columns \n",
    "distdf = distance_loc(latlongdf)\n",
    "distdf.index = distdf.columns\n",
    "distdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 17\n",
    "\n",
    "The subsequent function fills in the months that are missing from a certain location by averaging the rainfall amounts for the missing month in surrounding locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:19:13.612405Z",
     "start_time": "2019-09-08T15:19:06.469809Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def missingfillsurrounding(rdf, ddf):\n",
    "    ''' args: rdf = rainfall dataframe\n",
    "              ddf = distance df\n",
    "        first finds any locations that are missing data at any point in the column \n",
    "        then from that list it loops over it to find the locations that are within 85 kilometers\n",
    "        then creates a new datapoint from the mean of the surrounding locations. \n",
    "        returns: dataframe of filled data\n",
    "    '''\n",
    "    locwmd = rdf.columns[rdf.isna().any()].tolist()\n",
    "    for loc in locwmd:\n",
    "        nbloc = rdf[ddf[[loc]][ddf[loc] <=85].index]\n",
    "        missing = nbloc.index[nbloc[loc].isnull()]\n",
    "        if len(missing) >0:\n",
    "            for m in missing:\n",
    "                newpt = np.nanmean(nbloc.loc[m])\n",
    "                rdf.loc[m,loc] = newpt\n",
    "    return(rdf)\n",
    "alldatadf_filled = missingfillsurrounding(alldatadf, distdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 18 \n",
    "\n",
    "Separation of locations into north carolina and the surrounding states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:19:17.671952Z",
     "start_time": "2019-09-08T15:19:17.656329Z"
    }
   },
   "outputs": [],
   "source": [
    "locations = alldatadf_filled.columns\n",
    "ncloc = locations[locations.str.endswith('NC')]\n",
    "valoc = locations[locations.str.endswith('VA')]\n",
    "scloc = locations[locations.str.endswith('SC')]\n",
    "galoc = locations[locations.str.endswith('GA')]\n",
    "tnloc = locations[locations.str.endswith('TN')]\n",
    "ncdatadf = alldatadf_filled[ncloc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 19\n",
    "\n",
    "Placing dataframes into csv files to be exported into other notebooks for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:19:23.671400Z",
     "start_time": "2019-09-08T15:19:22.936731Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csvfiles_to_create = [\n",
    "    { 'data': alldatadf_filled, 'dest': os.path.join(destdir,'rainfalldata.csv') },\n",
    "    { 'data': distdf, 'dest': os.path.join(destdir,'distances.csv') },\n",
    "    { 'data': ncdatadf, 'dest': os.path.join(destdir,'ncrainfalldata.csv') }\n",
    "]\n",
    "\n",
    "for i in range(len(csvfiles_to_create)):\n",
    "    csvfiles_to_create[i]['data'].to_csv(csvfiles_to_create[i]['dest'])\n",
    "    print(\"[DATA_WRANGLING] Created file {}\".format(csvfiles_to_create[i]['dest']), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:19:30.326021Z",
     "start_time": "2019-09-08T15:19:30.294760Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of target locations = tarloc\n",
    "# list of exo locations = exoloc\n",
    "# latitude, longitude df = lldf\n",
    "def exofind(lldf, tarloc, exoloc):\n",
    "    '''args: tarloc is the list of target locations \n",
    "             exoloc is the list of exo locations\n",
    "             lldf is the latitude, longitude dataframe \n",
    "       returns: dictionary with keys being the location and values being the exogenous variables\n",
    "    '''\n",
    "    tarexoloc = tarloc.append(exoloc)\n",
    "    tarexoll = lldf.loc[tarexoloc]\n",
    "    # using the distance_loc function defined earlier to find the distances between just the \n",
    "    # target locations and the exogenous locations\n",
    "    tartoexodist = distance_loc(tarexoll)\n",
    "    exodistances = tartoexodist[exoloc] #subset of just the exogenous distances to target\n",
    "    exodistances.index = tartoexodist.columns\n",
    "    exodist2 = exodistances.drop(exoloc,axis=0)\n",
    "    closeexo = exodist2[exodist2 <= 50] # subsetting only those locations that are within 50 kilometers of the \n",
    "                                            #target locations\n",
    "    closeexo1 = closeexo.dropna(how='all') #dropping missing values\n",
    "    closeexo2 = closeexo1.dropna(axis=1,how='all')\n",
    "    exo = {}\n",
    "    for i in closeexo2.index:\n",
    "        ex = closeexo2.loc[i][closeexo2.loc[i].notnull()].index.tolist()\n",
    "        exo[i]=ex\n",
    "    return(exo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 20\n",
    "\n",
    "creating the dictionary from the exogenous locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:19:56.632815Z",
     "start_time": "2019-09-08T15:19:34.158545Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exoloc = valoc.append(scloc)\n",
    "exoloc = exoloc.append(galoc)\n",
    "exoloc = exoloc.append(tnloc)\n",
    "exogen = exofind(latlongdf,ncloc,exoloc)\n",
    "exogen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 21\n",
    "\n",
    "storing the dictionary so that it can be retrieved from the other jupyter notebooks or other python programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:19:57.304671Z",
     "start_time": "2019-09-08T15:19:57.195299Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  %store exogen\n",
    "except NameError:             # For non-jupyter processing\n",
    "    exogen_json = json.dumps(exogen, sort_keys=True, indent=4)\n",
    "    exogen_filepath = os.path.join(destdir,\"exogen.json\")\n",
    "    f = open(exogen_filepath,\"w+\")\n",
    "    f.write(exogen_json+'\\n')\n",
    "    f.close()\n",
    "    print(\"[DATA_WRANGLING] Created file {}\".format(exogen_filepath), flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 571.61666,
   "position": {
    "height": "40px",
    "left": "1197.6px",
    "right": "20px",
    "top": "120px",
    "width": "307.767px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
