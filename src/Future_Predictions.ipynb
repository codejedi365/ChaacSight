{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for the next 50 years (2019 - 2069)\n",
    "Includes exogenous locations outside of North Carolina to improve model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:51:04.069356Z",
     "start_time": "2019-09-01T15:49:22.228823Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import math\n",
    "import copy\n",
    "import multiprocessing\n",
    "import traceback\n",
    "import hashlib\n",
    "import signal\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try: \n",
    "    __file__\n",
    "except:\n",
    "    curr_dir = os.path.abspath('')\n",
    "else:\n",
    "    curr_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    \n",
    "app_root = curr_dir if os.path.basename(curr_dir) != \"src\" else os.path.dirname(curr_dir)\n",
    "\n",
    "if getpass.getuser() == \"rainfalld\":  # systemd daemon\n",
    "    home = os.path.expanduser(\"~\")\n",
    "    destdir = home                    # /var/cache/rainfall-predictor\n",
    "else:\n",
    "    destdir = os.path.join(app_root,'data','manipulated_data')      # if dev, stay in repository\n",
    "\n",
    "\n",
    "file = os.path.join(destdir,'rainfalldata.csv')\n",
    "rd = pd.read_csv(file)\n",
    "file2 = os.path.join(destdir,'ncrainfalldata.csv')\n",
    "ncrd = pd.read_csv(file2)\n",
    "rd.Date = pd.to_datetime(rd.Date)\n",
    "rd = rd.set_index('Date')\n",
    "ncrd.Date = pd.to_datetime(ncrd.Date)\n",
    "ncrd = ncrd.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T10:30:04.299825Z",
     "start_time": "2019-08-20T10:30:04.206119Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# this cell takes the stored exogen dictionary that is stored in the Data_Wrangling_CAP1 jupyter notebook\n",
    "# that was imported above.\n",
    "try:\n",
    "    raise NameError()\n",
    "    %store -r exogen\n",
    "except NameError:\n",
    "    f = open(os.path.join(destdir,\"exogen.json\"),\"r\")\n",
    "    exogen = json.load(f)      # read from file, passed from Data_Wrangling\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Library\n",
    "Handles ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:51:04.428724Z",
     "start_time": "2019-09-01T15:51:04.413112Z"
    }
   },
   "outputs": [],
   "source": [
    "def sarima_model_creation(data, p, d, q, P, D, Q, m, exog=None):\n",
    "    my_order = [p,d,q]\n",
    "    my_sorder = [P,D,Q,m]\n",
    "    sarimamod = sm.tsa.statespace.SARIMAX(data, exog, order=my_order, seasonal_order=my_sorder, \n",
    "                                          enforce_stationarity=False, enforce_invertibility=False,\n",
    "                                          initialization='approximate_diffuse')\n",
    "    model_fit = sarimamod.fit(disp=0)   # start_params=[0, 0, 0, 0, 1])\n",
    "    return(model_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T13:02:33.081089Z",
     "start_time": "2019-09-01T13:02:33.034217Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_creation_pred_one_step(train_data, test_data, exotrain=None, exotest=None, progress_bar=None):\n",
    "    ''' recursively makes forecast based on provided data for the next month\n",
    "        args: train_data = large data set to base predictions on\n",
    "              test_data  = decreasing dataset of data to test model\n",
    "              exotrain   = exogenous location data that matches the same timeframe of train_data but was not included\n",
    "              exotest    = exogenous location data that matches the same timeframe of test_data but was not included\n",
    "        returns: A list of all predictions for the location matching the entire test_data timeframe\n",
    "    '''\n",
    "    list_one_step = []\n",
    "    \n",
    "    nextMonth = model_based_forecast(train_data, exotrain)\n",
    "    list_one_step.append(nextMonth[0])             # captures prediction\n",
    "    progress_bar.update()\n",
    "\n",
    "    # if test data exists\n",
    "    if len(test_data) > 1:\n",
    "        # increment data for next month's iteration\n",
    "        train_data = pd.concat([train_data, test_data.iloc[[0]]])\n",
    "        test_data = test_data.drop(test_data.index[0], axis = 0)\n",
    "        if exotrain is not None:\n",
    "            exotrain = pd.concat([exotrain, exotest.iloc[[0]]])\n",
    "            exotest = exotest.drop(exotest.index[0], axis = 0)\n",
    "\n",
    "        # execute & capture future predictions\n",
    "        futurePredictions = model_creation_pred_one_step(train_data, test_data, exotrain, exotest, progress_bar)\n",
    "        # add to list\n",
    "        list_one_step.extend(futurePredictions)\n",
    "        \n",
    "    return(list_one_step)\n",
    "\n",
    "def model_based_forecast(train_data, exotrain=None):\n",
    "    ''' creates model from training data & makes a forecast\n",
    "        args: train_data = DataFrame to build forecasting model\n",
    "              exotrain   = DataFrame of exogenous location's rainfall data\n",
    "        returns: FLOAT value of next month's forecast value\n",
    "    '''\n",
    "    mod = sarima_model_creation(train_data, p=4, d=0, q=3, P=3, D=0, Q=4, m=12, exog=exotrain)\n",
    "    # if exists, passing exotrain's prevMonth (december, for forecasting jan), otherwise only forcast based on model\n",
    "    nextMonth = mod.forecast() if exotrain is None else mod.forecast(exog=exotrain.iloc[[-1]])       # turnary assignment expression\n",
    "    return(nextMonth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_JSON_file(filename, adjustfn, arglist=(), kwargs={}, sort=True):\n",
    "    ''' Generic function to handle JSON file updates.  Reads-in entire file, \n",
    "        federates out updates with adjustment fn's, and then overwrites original file completely\n",
    "        Handles FileNotFoundError & JSONDecodeError automatically.\n",
    "        args: filename = json-encoded file on disk\n",
    "              adjustfn = function to perform adjustments to loaded dictionary file\n",
    "              arglist = positional args to pass on to adjustfn\n",
    "              kwargs = keyword args to pass on to adjustfn\n",
    "              sort = flag to auto-sort keys when saving to file [Default = True]\n",
    "        returns: dictionary object that was updated and saved to file\n",
    "    '''\n",
    "    def default_dict_adjustfn(data, key, value):\n",
    "        ''' Generic default function for updating a basic dictionary data file (top level keys only)\n",
    "            args: data = dictionary representation of JSON data from file\n",
    "                  key = key name to enter into dictionary\n",
    "                  value = value to enter into dictionary[key]\n",
    "            returns: Updated dictionary with key/value added\n",
    "        '''\n",
    "        data[key] = value\n",
    "        return(data)\n",
    "    \n",
    "    def default_list_adjustfn(data, value):\n",
    "        ''' Generic default function for updating a basic list data file (add to bottom of list)\n",
    "            args: data = list representation of JSON data from file\n",
    "                  value = value to append to end of list, list[len(list)] = value\n",
    "            returns: Updated list with value appended\n",
    "        '''\n",
    "        data.append(value)\n",
    "        return(data)\n",
    "    \n",
    "    loaded = False\n",
    "    while not loaded:\n",
    "        try:\n",
    "            file = open(filename, \"r+\")\n",
    "            json_data = json.loads(file.read())\n",
    "        except FileNotFoundError:\n",
    "            open(filename, \"w+\").close()       # create file on disk\n",
    "            continue\n",
    "        except json.JSONDecodeError:\n",
    "            json_data = {}\n",
    "        \n",
    "        loaded = True\n",
    "        if adjustfn is not None:\n",
    "            json_data = adjustfn(json_data, *arglist, **kwargs)\n",
    "        else:\n",
    "            if isinstance(json_data, dict):\n",
    "                json_data = default_dict_adjustfn(json_data, *arglist, **kwargs)\n",
    "            elif isinstance(json_data, list):\n",
    "                json_data = default_list_adjustfn(json_data, *arglist, **kwargs)\n",
    "            else:\n",
    "                raise ValueError('Unable to adjust JSON since function not provided or file not of type dict or list!')\n",
    "        \n",
    "        file.seek(0)                           # Go to first line, first column of file\n",
    "        file.write( json.dumps(json_data, sort_keys=sort, indent=4)+'\\n')\n",
    "        file.truncate()                        # end file here, delete anything after the current file position\n",
    "        file.close()\n",
    "    \n",
    "    return(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bettermae_results_filename = os.path.join(destdir,\"allBetterMAE.json\")\n",
    "\n",
    "files=[]\n",
    "while (len(files) > 0):                          # reset results on new run\n",
    "    try:\n",
    "        os.remove( files[-1] )\n",
    "    except FileNotFoundError:                    # ignore since non-exist is the desired state\n",
    "        pass\n",
    "    except OSError as err:\n",
    "        traceback.print_exception(type(err), err, err.__traceback__)\n",
    "    finally:\n",
    "        files.pop()\n",
    "\n",
    "\n",
    "data_test_percentage = 0.2                                           # 20%\n",
    "num_single_predictions = math.ceil(rd.shape[0]*data_test_percentage)\n",
    "num_all_predictions = len(l_o_dfs.items())*num_single_predictions    # keymae predictions amount\n",
    "# num_all_exmae = 0\n",
    "# for key,value in l_o_dfs.items():\n",
    "#     num_all_exmae += len(value)                                      # exmae predictions amount\n",
    "# num_all_predictions += num_all_exmae*num_single_predictions\n",
    "\n",
    "# keymae_storage = {}    \n",
    "# # tqdmformat = '{desc}: |{bar}|{percentage:3.0f}%'\n",
    "# total_progress = tqdm(desc=\"Full Calculation:\", total=num_all_predictions, position=0)\n",
    "# keymae_progress = tqdm(desc=\"Finding keymaes:\", total=len(l_o_dfs.items()), position=1)\n",
    "# exmae_progress = tqdm(desc=\"Evaluating exmaes:\", total=num_all_exmae, position=2)\n",
    "\n",
    "# try:\n",
    "#     # Solve for targetloc Keymae Values first\n",
    "#     print(\"============== KEYMAE EVALUATION ===============\")\n",
    "#     pbars = { 'total_pbar': total_progress, 'keymae_pbar': keymae_progress, 'exmae_pbar': exmae_progress }\n",
    "#     keymae_storage = targetloc_vars(rd, list(l_o_dfs.keys()), data_test_percentage, pbars)\n",
    "\n",
    "#     # Solve for exmae values of each combination of targetloc and matching exogenous variable\n",
    "#     print(\"\\n=============== EXMAE EVALUATION ===============\")\n",
    "#     for key,value in l_o_dfs.items():\n",
    "#         print(\"\\nFinding exmaes values for {0}:\".format(key))\n",
    "#         targetloc_obj = { 'data': rd[key], 'name': key, 'keymae': keymae_storage[key] }\n",
    "#         exogenous_var(targetloc_obj, data_test_percentage, value, pbars)\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"MANUAL EXIT: Program interrupted by user.\", flush=True)\n",
    "#     raise SystemExit(2)\n",
    "# except Exception as err:\n",
    "#     print(\"ERROR: {}\".format(err), flush=True)\n",
    "#     traceback.print_exception(type(err), err, err.__traceback__)\n",
    "#     raise SystemExit(1)\n",
    "# else:\n",
    "#     print(\"\\n==== EXOGENOUS VARIABLE EVALUATION COMPLETE ====\\n\")\n",
    "# finally:\n",
    "#     keymae_progress.close()\n",
    "#     exmae_progress.close()\n",
    "#     total_progress.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load any found exogenous locations that improve model accuracy for certain cities\n",
    "try:\n",
    "    with open(bettermae_results_filename, \"r\") as f:\n",
    "        improvement_exog = json.loads(f.read())\n",
    "except FileNotFoundError as err:\n",
    "    print(\"File ({0}) cannot be found. Exiting...\".format(bettermae_results_filename))\n",
    "    SystemExit(1)\n",
    "except json.JSONDecodeError as err:\n",
    "    print(\"File ({0}) has been corrupted. Cannot read json.\".format(bettermae_results_filename))\n",
    "    print(\"Exiting...\")\n",
    "    SystemExit(1)\n",
    "else:\n",
    "    with_exogs = list(improvement_exog.keys())    # Extract target locations with exogenous locations that improve the target predictions\n",
    "    ncrd2 = ncrd.copy()\n",
    "    ncrd_less = ncrd2.drop(with_exogs,axis=1)     # Separate out locations with exogenous locations from other data\n",
    "    print(\"with_exogs = {0}\".format(with_exogs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency Sort\n",
    "# if a city's prediction model is improved by an exogenous location than it is dependent on the exogenous location's predictions\n",
    "# Cities without exogenous locations are least dependent so they will be solved first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_fx(data, begin, end):\n",
    "    base = datetime.strptime(begin,'%Y-%m-%d')\n",
    "    date_list = [base + relativedelta(months=x) for x in range(600)]\n",
    "    prediction1_df = pd.DataFrame(index=date_list)\n",
    "    for col in tqdm(data.columns):\n",
    "        loc = data[col]\n",
    "        mod_fit1 = sarima_model_creation(loc, 4,0,3,3,0,4,12)\n",
    "        point_predictions = pd.DataFrame(mod_fit1.predict(start=begin, end=end), columns=[col])\n",
    "        future_pred1 = mod_fit1.get_prediction(start=begin, end=end)\n",
    "        future_pred1_ci = future_pred1.conf_int(alpha=0.5)\n",
    "        point_predictions_df = pd.merge(point_predictions, future_pred1_ci, left_index=True, right_index=True)\n",
    "        prediction1_df = pd.merge(prediction1_df, point_predictions_df, left_index=True, right_index=True)\n",
    "    return(prediction1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_exog_fx2(data, exog_dict, begin, end):\n",
    "    base = datetime.strptime(begin,'%Y-%m-%d')\n",
    "    end_date = datetime.strptime(begin,'%Y-%m-%d')\n",
    "    num_predicted_months = (base.year - end_date.year) * 12 + base.month - end_date.month\n",
    "    date_list = [base + relativedelta(months=x) for x in range(num_predicted_months)]\n",
    "    prediction_df = pd.DataFrame(index = date_list)\n",
    "    pred_val_df = pd.DataFrame(index = date_list)\n",
    "    exog_predictions_df = pd.DataFrame(index = date_list)\n",
    "    for key,value in tqdm(exog_dict.items()):\n",
    "        loc = data[key]\n",
    "        mod_fit1 = sarima_model_creation(loc, 4,0,3,3,0,4, 12,exog=value)\n",
    "        if value.shape[1] > 1:\n",
    "            shap = value.shape[1]\n",
    "            for i in range(shap):\n",
    "                exog_mod_fit = sarima_model_creation(value.iloc[:,i],4,0,3,3,0,4,12)\n",
    "                e_preds2 = pd.DataFrame(exog_mod_fit.predict(start=begin, end=end))\n",
    "                if i is 0:\n",
    "                    exog_predictions_df = e_preds2\n",
    "                else:\n",
    "                    exog_predictions_df = pd.merge(exog_predictions_df, e_preds2, left_index=True, \n",
    "                                                   right_index=True)\n",
    "        else:\n",
    "            exog_mod_fit = sarima_model_creation(value, 4,0,3,3,0,4,12)\n",
    "            exog_predictions_df = pd.DataFrame(exog_mod_fit.predict(start=begin, end=end))\n",
    "        future_pred = mod_fit1.get_prediction(exog=exog_predictions_df,start=begin, end=end)\n",
    "        future_pred_ci = future_pred.conf_int(alpha=0.5)\n",
    "        future_pred_val= pd.DataFrame(mod_fit1.predict(exog=exog_predictions_df, start=begin, end=end), \n",
    "                                      columns = [key])\n",
    "        future_pred_full = pd.merge(future_pred_val, future_pred_ci, left_index=True, right_index=True)\n",
    "        prediction_df = pd.merge(prediction_df, future_pred_full, left_index=True, right_index=True)\n",
    "    return(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary of dataframes containing exogenous location rainfall data organized by target location\n",
    "exo_var_dict2 = {}\n",
    "for i in range(0,len(with_exogs)):\n",
    "    targetloc = with_exogs[i]\n",
    "    bestmae_combo = None\n",
    "    for combo,mae in improvement_exog[targetloc]['exogen'].items():\n",
    "        if bestmae_combo is None:                         # BubbleSort always starts arbitrary\n",
    "            bestmae_combo = combo\n",
    "        elif mae < improvement_exog[targetloc]['exogen'][bestmae_combo]:    # Found better MAE\n",
    "            bestmae_combo = combo\n",
    "        else:\n",
    "            pass\n",
    "    # with best combo, split if it uses multiple exog_vars\n",
    "    combo_exogloc = bestmae_combo.split('|')\n",
    "    exog_raindata = rd[[combo_exogloc[0]]]\n",
    "    for e in range(1,len(combo_exogloc)):\n",
    "        exog_raindata = pd.merge(exog_raindata, rd[[combo_exogloc[e]]], left_index=True, right_index=True)\n",
    "    exo_var_dict2[targetloc] = exog_raindata\n",
    "\n",
    "print(exo_var_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS_IN_FUTURE = 50\n",
    "\n",
    "last_date_in_data = (rd.iloc[[-1]]).index[0].to_pydatetime()                   # extract last date in DataFrame\n",
    "date_lowerbound = last_date_in_data + relativedelta(months=1)                  # next month\n",
    "date_upperbound = last_date_in_data + relativedelta(years=YEARS_IN_FUTURE)     # future date to predict to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = prediction_fx(ncrd_less, date_lowerbound.date(), date_upperbound.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_ci_df = prediction_exog_fx2(rd, exo_var_dict2, date_lowerbound.date(), date_upperbound.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ci_vals = pd.merge(pre_df, e_ci_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ci_vals.to_csv(os.path.join(destdir,'predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
